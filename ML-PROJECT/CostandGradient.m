function [J grad] = CostandGradient(initialweights, ...                                   input_layer_size, ...                                   hidden_layer1_size, ...                                   hidden_layer2_size,no_labels, ...                                   X, y, lambda)%reshaping our theta parameters into their original dimentionsTheta1 = reshape(initialweights(1:hidden_layer1_size * (input_layer_size + 1)), ...                 hidden_layer1_size, (input_layer_size + 1));Theta2 = reshape(initialweights(1+(hidden_layer1_size * (input_layer_size + 1)):...                 (hidden_layer2_size * (hidden_layer1_size+ 1)+hidden_layer1_size * (input_layer_size + 1))), ...                 hidden_layer2_size, (hidden_layer1_size + 1));Theta3 = reshape(initialweights((1+(hidden_layer1_size * (input_layer_size + 1))+hidden_layer2_size * (hidden_layer1_size+ 1)):end), ...                 no_labels, (hidden_layer2_size + 1));m = size(X, 1);%initializing the return valuesJ = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));Theta3_grad = zeros(size(Theta3));h=zeros(m,no_labels);c=modifylabels(y,no_labels);y=c;X=[ones(m,1) X];%adding bias termdel1=zeros(size(Theta1));del2=zeros(size(Theta2));del3=zeros(size(Theta3));%training the neural networkfor i=1:m,  %forward propagation  a1=X(i,:)';  a2=sigmoid(Theta1*a1);  a2=[1;a2];  a3=sigmoid(Theta2*a2);  a3=[1;a3];  a4=sigmoid(Theta3*a3);  %[maxval maxind]=max(a4);  h(i,:)=a4';  %back propagation  d4=(h(i,:)-y(i,:))';  d3=((Theta3'*d4)(2:end)).*sigmoidGradient(Theta2*a2);  d2=((Theta2'*d3)(2:end)).*sigmoidGradient(Theta1*a1);  del1=del1+(d2*a1');  del2=del2+(d3*a2');  del3=del3+(d4*a3');endTheta1_grad(:,1)=(1/m)*(del1(:,1));Theta1_grad(:,(2:end))=(1/m)*(del1(:,(2:end)))+(lambda/m)*(Theta1(:,(2:end)));Theta2_grad(:,1)=(1/m)*(del2(:,1));Theta2_grad(:,(2:end))=(1/m)*(del2(:,(2:end)))+(lambda/m)*(Theta2(:,(2:end)));Theta3_grad(:,1)=(1/m)*(del3(:,1));Theta3_grad(:,(2:end))=(1/m)*(del3(:,(2:end)))+(lambda/m)*(Theta3(:,(2:end)));%computing total error(cost) with regularizationk=zeros(m,1);for i=1:m,  k(i,:)=(-1/m)*((y(i,:)*log(h(i,:)'))+((1-y(i,:))*log(1-(h(i,:)'))));endreg=(lambda/(2*m))*((sum(sum((Theta1(:,[2:end]).^2))))+(sum(sum((Theta2(:,[2:end]).^2)))));%regularization term-L2 regularizationJ=sum(k)+reg;% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:); Theta3_grad(:)];end